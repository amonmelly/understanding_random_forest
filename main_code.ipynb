{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c7149146",
   "metadata": {},
   "source": [
    "                                Prepared by: Amon Melly || Email: amon.kmelly@gmail.com\n",
    "## Decision Tree & Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85d0c24e",
   "metadata": {},
   "source": [
    "### Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "79200ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Processing\n",
    "import pandas as pd\n",
    "\n",
    "# Modelling\n",
    "from sklearn.model_selection import train_test_split,RandomizedSearchCV\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import r2_score\n",
    "from scipy.stats import randint\n",
    "from skopt import BayesSearchCV\n",
    "\n",
    "\n",
    "# Tree Visualisation\n",
    "from sklearn.tree import export_graphviz\n",
    "from IPython.display import Image\n",
    "import graphviz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08ecfe40",
   "metadata": {},
   "source": [
    "### Preprocessing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "5a73803e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('Maji_Ndogo_agric_survey_data_small.csv')\n",
    "\n",
    "# Convert categorical variable 'Soil_type' into dummy variables\n",
    "dummies = pd.get_dummies(data['Soil_type'], prefix='Soil_type')\n",
    "\n",
    "# Concatenate the dummy variables with the original DataFrame excluding 'Soil_type'\n",
    "data_with_dummies = pd.concat([data.drop(['Soil_type'], axis =1), dummies], axis=1)\n",
    "\n",
    "# Convert Boolean columns to integer\n",
    "for col in data_with_dummies.columns:\n",
    "    if data_with_dummies[col].dtype == 'bool':\n",
    "        data_with_dummies[col] = data_with_dummies[col].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "5165abdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_with_dummies.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5270eb12",
   "metadata": {},
   "source": [
    "### Splitting the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "6e1cae9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into features (X) and target (y)\n",
    "X = data_with_dummies.drop('Standard_yield', axis=1)\n",
    "y = data_with_dummies['Standard_yield']\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebf7396e",
   "metadata": {},
   "source": [
    "### 1. Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "7a4b7820",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DecisionTreeRegressor(random_state=42, max_depth = 4)\n",
    "model.fit(X_train, y_train)\n",
    "predicted = model.predict(X_test)\n",
    "\n",
    "#R Squared Score\n",
    "print(f'R_Squared Score: {r2_score(y_test,predicted)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd7834bc",
   "metadata": {},
   "source": [
    "### Visualizing your Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "e77ef90b",
   "metadata": {},
   "outputs": [],
   "source": [
    "XX = data_with_dummies[['Pollution_level','Rainfall','pH']]\n",
    "yy = data_with_dummies['Standard_yield']\n",
    "\n",
    "model_vis = DecisionTreeRegressor(random_state=42, max_depth = 2)\n",
    "model_vis.fit(XX,yy)\n",
    "\n",
    "dot_data = export_graphviz(model_vis,\n",
    "                           feature_names=XX.columns,\n",
    "                           filled=True,\n",
    "                           max_depth=2,\n",
    "                           impurity=False,\n",
    "                           proportion=True)\n",
    "graph = graphviz.Source(dot_data)\n",
    "display(graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d17c4603",
   "metadata": {},
   "source": [
    "### The concept of overfitting the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "c228bd19",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adjusting max_depth between 2 & 15\n",
    "\n",
    "print('\\nR_Squared Score Analysis\\n')\n",
    "\n",
    "for max_depth in range(2,16):\n",
    "    model_over = DecisionTreeRegressor(random_state=42, max_depth = max_depth)\n",
    "    model_over.fit(X_train, y_train)\n",
    "    test_prediction = model_over.predict(X_test)\n",
    "    train_prediction = model_over.predict (X_train)\n",
    "    print(f'New unseen data:={round(r2_score(y_test,test_prediction),4)}, Train set:={round(r2_score(y_train,train_prediction),4)}, Maximum depth={max_depth}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc7973c1",
   "metadata": {},
   "source": [
    "### Pruning the tree\n",
    "**Types of Pruning**\n",
    "- **`Pre-Pruning (Early Stopping):`** Stop growing the tree before it reaches its maximum depth or minimum sample size criteria during training.\n",
    "- **`Post-Pruning (Reduced Error Pruning):`** Grow the tree to its full extent during training, then selectively remove branches that lead to minimal improvements in model performance.\n",
    "<br></br>\n",
    "We will be using Cost Complexity Pruning (CCP) which is a post-pruning technique. This is a technique used to prune decision trees, optimizing their complexity for better generalization.\n",
    "<br></br>\n",
    "**What is Cost Complexity Pruning?**\n",
    "Cost Complexity Pruning is a method for reducing the complexity of decision trees by systematically removing nodes based on a cost complexity measure.\n",
    "It involves finding the optimal trade-off between tree complexity and predictive accuracy.\n",
    "<br></br>\n",
    "**Understanding Alphas**\n",
    "- Alphas represent the complexity parameter used in Cost Complexity Pruning.\n",
    "- Higher alphas result in more aggressive pruning, leading to simpler trees with fewer nodes.\n",
    "- Lower alphas lead to less pruning, resulting in more complex trees with more nodes.\n",
    "<br></br>\n",
    "**How Cost Complexity Pruning Works**\n",
    "- The Cost Complexity Pruning Path computes the effective alphas of decision tree nodes during pruning.\n",
    "- It identifies which nodes to prune based on their cost complexity measures.\n",
    "- The path provides insights into how the complexity of the tree changes as nodes are pruned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "724d0fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform post-pruning (reduced error pruning)\n",
    "path = model.cost_complexity_pruning_path(X_train, y_train)\n",
    "ccp_alphas = path.ccp_alphas\n",
    "regressors = []\n",
    "\n",
    "for ccp_alpha in ccp_alphas:\n",
    "    regressor = DecisionTreeRegressor(random_state=42, ccp_alpha=ccp_alpha)\n",
    "    regressor.fit(X_train, y_train)\n",
    "    regressors.append(regressor)\n",
    "\n",
    "# Find the optimal subtree by selecting the model with the highest r2\n",
    "r2 = [r2_score(y_test, regressor.predict(X_test)) for regressor in regressors]\n",
    "best_index = r2.index(max(r2))\n",
    "best_regressor = regressors[best_index]\n",
    "\n",
    "# Evaluate the pruned regressor on the testing set\n",
    "y_pred_pruned = best_regressor.predict(X_test)\n",
    "r2_ = r2_score(y_test, y_pred_pruned)\n",
    "print(\"R_Squared after pruning:\", r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5414d4bf",
   "metadata": {},
   "source": [
    "### Feature Importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "b679d428",
   "metadata": {},
   "outputs": [],
   "source": [
    "importances = pd.DataFrame(best_regressor.feature_importances_, columns = ['Importance_Score'],\n",
    "                           index=X_train.columns).sort_values(by='Importance_Score', ascending = False)\n",
    "importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "b7bf681b",
   "metadata": {},
   "outputs": [],
   "source": [
    "importances['Importance_Score'].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06abc708",
   "metadata": {},
   "source": [
    "### 2. Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "4cc20e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestRegressor(random_state=42, max_depth = 5,n_estimators=100)\n",
    "rf.fit(X_train, y_train)\n",
    "predicted = rf.predict(X_test)\n",
    "\n",
    "#R Squared Score\n",
    "print(f'R_Squared Score: {r2_score(y_test,predicted)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c842761",
   "metadata": {},
   "source": [
    "### Tuning Hyperparameters\n",
    "The process of selecting the optimal set of hyperparameters for a machine learning model to improve its performance.\n",
    "### 1. Manual Search\n",
    "Manual adjustment of hyperparameters based on domain knowledge and experimentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "75ec7ec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adjusting max_depth between 3 & 10 while keeping n_estimators constant\n",
    "\n",
    "print('\\nR_Squared Score Analysis\\n')\n",
    "\n",
    "for max_depth in range(3,11):\n",
    "    rf_ = RandomForestRegressor(random_state=42, max_depth = max_depth, n_estimators=100)\n",
    "    rf_.fit(X_train, y_train)\n",
    "    test_prediction = rf_.predict(X_test)\n",
    "    train_prediction = rf_.predict (X_train)\n",
    "    print(f'New unseen data:={round(r2_score(y_test,test_prediction),4)}, Train set:={round(r2_score(y_train,train_prediction),4)}, Maximum depth={max_depth}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "6ba68fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adjusting n_estimators between 50 & 200 while keeping max_depth constant\n",
    "\n",
    "print('\\nR_Squared Score Analysis\\n')\n",
    "\n",
    "for n in range(50,201,50):\n",
    "    rf_ = RandomForestRegressor(random_state=42, max_depth = 8, n_estimators=n)\n",
    "    rf_.fit(X_train, y_train)\n",
    "    test_prediction = rf_.predict(X_test)\n",
    "    train_prediction = rf_.predict (X_train)\n",
    "    print(f'New unseen data:={round(r2_score(y_test,test_prediction),4)}, Train set:={round(r2_score(y_train,train_prediction),4)}, Number of estimators={n}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f04d5d06",
   "metadata": {},
   "source": [
    "### 2. RandomizedSearchCV\n",
    "Randomly samples a fixed number of hyperparameter settings from a predefined distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "3856d6f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_dist = {'n_estimators': randint(50,200),\n",
    "              'max_depth': randint(3,15)}\n",
    "\n",
    "# Create a random forest regressor\n",
    "rf_ = RandomForestRegressor()\n",
    "\n",
    "# Use random search to find the best hyperparameters\n",
    "rand_search = RandomizedSearchCV(rf_, \n",
    "                                 param_distributions = param_dist, \n",
    "                                 n_iter=5, \n",
    "                                 cv=5)\n",
    "\n",
    "# Fit the random search object to the data\n",
    "rand_search.fit(X_train, y_train)\n",
    "\n",
    "# Create a variable for the best model\n",
    "best_rf = rand_search.best_estimator_\n",
    "\n",
    "# Print the best hyperparameters\n",
    "print('Best hyperparameters:',  rand_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "245a8ae2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R_Squared Score: 0.5372439684958958\n"
     ]
    }
   ],
   "source": [
    "#Checking score of the tuned paramaters\n",
    "parameters = rand_search.best_params_\n",
    "\n",
    "rf_ = RandomForestRegressor(random_state=42, max_depth = parameters['max_depth'], n_estimators=parameters['n_estimators'])\n",
    "rf_.fit(X_train, y_train)\n",
    "predicted = rf_.predict(X_test)\n",
    "\n",
    "#Mean Squared Error\n",
    "#print(f'R_Squared Score: {r2_score(y_test,predicted)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "332db0d3",
   "metadata": {},
   "source": [
    "### 3. Bayesian Optimization\n",
    "- Bayesian optimization is an iterative model-based optimization technique that uses probabilistic models to find the optimal set of hyperparameters.\n",
    "- It efficiently explores the hyperparameter space by selecting the next set of hyperparameters based on the results of previous evaluations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "1e0649c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-5 {color: black;}#sk-container-id-5 pre{padding: 0;}#sk-container-id-5 div.sk-toggleable {background-color: white;}#sk-container-id-5 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-5 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-5 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-5 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-5 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-5 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-5 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-5 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-5 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-5 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-5 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-5 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-5 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-5 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-5 div.sk-item {position: relative;z-index: 1;}#sk-container-id-5 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-5 div.sk-item::before, #sk-container-id-5 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-5 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-5 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-5 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-5 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-5 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-5 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-5 div.sk-label-container {text-align: center;}#sk-container-id-5 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-5 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-5\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>BayesSearchCV(cv=5, estimator=RandomForestRegressor(), n_iter=5, n_jobs=-1,\n",
       "              random_state=42, scoring=&#x27;neg_mean_squared_error&#x27;,\n",
       "              search_spaces={&#x27;max_depth&#x27;: (1, 15), &#x27;max_features&#x27;: (0.1, 1.0),\n",
       "                             &#x27;min_samples_leaf&#x27;: (1, 10),\n",
       "                             &#x27;min_samples_split&#x27;: (2, 20),\n",
       "                             &#x27;n_estimators&#x27;: (3, 200)})</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" ><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">BayesSearchCV</label><div class=\"sk-toggleable__content\"><pre>BayesSearchCV(cv=5, estimator=RandomForestRegressor(), n_iter=5, n_jobs=-1,\n",
       "              random_state=42, scoring=&#x27;neg_mean_squared_error&#x27;,\n",
       "              search_spaces={&#x27;max_depth&#x27;: (1, 15), &#x27;max_features&#x27;: (0.1, 1.0),\n",
       "                             &#x27;min_samples_leaf&#x27;: (1, 10),\n",
       "                             &#x27;min_samples_split&#x27;: (2, 20),\n",
       "                             &#x27;n_estimators&#x27;: (3, 200)})</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-6\" type=\"checkbox\" ><label for=\"sk-estimator-id-6\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: RandomForestRegressor</label><div class=\"sk-toggleable__content\"><pre>RandomForestRegressor()</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-7\" type=\"checkbox\" ><label for=\"sk-estimator-id-7\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestRegressor</label><div class=\"sk-toggleable__content\"><pre>RandomForestRegressor()</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "BayesSearchCV(cv=5, estimator=RandomForestRegressor(), n_iter=5, n_jobs=-1,\n",
       "              random_state=42, scoring='neg_mean_squared_error',\n",
       "              search_spaces={'max_depth': (1, 15), 'max_features': (0.1, 1.0),\n",
       "                             'min_samples_leaf': (1, 10),\n",
       "                             'min_samples_split': (2, 20),\n",
       "                             'n_estimators': (3, 200)})"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the Random Forest Regressor model\n",
    "rf_regressor = RandomForestRegressor()\n",
    "\n",
    "# Define the search space for hyperparameters\n",
    "search_space = {\n",
    "    'n_estimators': (3, 200),          # Number of trees in the forest\n",
    "    'max_depth': (1, 15),                # Maximum depth of each tree\n",
    "    'min_samples_split': (2, 20),        # Minimum number of samples required to split an internal node\n",
    "    'min_samples_leaf': (1, 10),         # Minimum number of samples required to be at a leaf node\n",
    "    'max_features': (0.1, 1.0)           # Number of features to consider when looking for the best split\n",
    "}\n",
    "\n",
    "# Perform Bayesian Optimization\n",
    "bayes_search = BayesSearchCV(\n",
    "    estimator=rf_regressor,\n",
    "    search_spaces=search_space,\n",
    "    scoring='neg_mean_squared_error',\n",
    "    cv=5,\n",
    "    n_iter=5,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Fit the Bayesian Optimization model\n",
    "bayes_search.fit(X_train, y_train)\n",
    "\n",
    "# Print the best hyperparameters\n",
    "print(\"Best hyperparameters:\", bayes_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "42c152c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking score of the tuned paramaters\n",
    "parameters = bayes_search.best_params_\n",
    "\n",
    "rf_ = RandomForestRegressor(random_state=42,\n",
    "                            max_depth = parameters['max_depth'],\n",
    "                            n_estimators = parameters['n_estimators'],\n",
    "                            max_features = parameters['max_features'],\n",
    "                            min_samples_leaf = parameters['min_samples_leaf'],\n",
    "                            min_samples_split = parameters['min_samples_split']\n",
    "                           )\n",
    "rf_.fit(X_train, y_train)\n",
    "predicted = rf_.predict(X_test)\n",
    "\n",
    "#Mean Squared Error\n",
    "print(f'R_Squared Score: {r2_score(y_test,predicted)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "297d2584",
   "metadata": {},
   "source": [
    "### Out-of-bag evaluation\n",
    "The out-of-bag (OOB) error in random forests is an estimate of the model's performance on unseen data without the need for a separate validation set. It is a useful metric for assessing the generalization ability of a random forest model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "582153db",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_regressor = RandomForestRegressor(random_state=42,\n",
    "                                     max_depth = parameters['max_depth'],\n",
    "                                     n_estimators = parameters['n_estimators'],\n",
    "                                     max_features = parameters['max_features'],\n",
    "                                     min_samples_leaf = parameters['min_samples_leaf'],\n",
    "                                     min_samples_split = parameters['min_samples_split'],\n",
    "                                     oob_score=True\n",
    "                                    )\n",
    "rf_regressor.fit(X_train, y_train)\n",
    "\n",
    "# Compute the OOB error\n",
    "oob_error = 1 - rf_regressor.oob_score_\n",
    "print(\"Out-of-Bag Error:\", oob_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38bb96e4",
   "metadata": {},
   "source": [
    "### Question\n",
    "#### In Decision tree we did visualize the tree structure, is it possible to do the same for random forest?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
